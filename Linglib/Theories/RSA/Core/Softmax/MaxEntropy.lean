import Linglib.Theories.RSA.Core.Softmax.Basic
import Mathlib.Analysis.Convex.Mul
import Mathlib.Analysis.SpecialFunctions.Log.Deriv

/-!
# Softmax as Maximum Entropy Distribution

Softmax maximizes entropy-regularized objective: argmax_p [‚ü®s, p‚ü© + (1/Œ±) H(p)].
-/

namespace Softmax

open Real BigOperators Finset

variable {Œπ : Type*} [Fintype Œπ] [DecidableEq Œπ] [Nonempty Œπ]

/-- Shannon entropy: H(p) = -Œ£·µ¢ p·µ¢ log p·µ¢. -/
noncomputable def shannonEntropy (p : Œπ ‚Üí ‚Ñù) : ‚Ñù :=
  -‚àë i : Œπ, if p i = 0 then 0 else p i * log (p i)

/-- Entropy is non-negative for probability distributions. -/
theorem shannonEntropy_nonneg (p : Œπ ‚Üí ‚Ñù)
    (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1) :
    0 ‚â§ shannonEntropy p := by
  simp only [shannonEntropy]
  rw [neg_nonneg]
  apply Finset.sum_nonpos
  intro i _
  by_cases hi : p i = 0
  ¬∑ simp [hi]
  ¬∑ simp only [hi, ‚ÜìreduceIte]
    have hp_pos : 0 < p i := (hp_nonneg i).lt_of_ne' hi
    have hp_le : p i ‚â§ 1 := by
      calc p i ‚â§ ‚àë j : Œπ, p j := Finset.single_le_sum (Œª j _ => hp_nonneg j) (Finset.mem_univ i)
        _ = 1 := hp_sum
    have hlog : log (p i) ‚â§ 0 := log_nonpos (le_of_lt hp_pos) hp_le
    exact mul_nonpos_of_nonneg_of_nonpos (le_of_lt hp_pos) hlog

/-- Maximum entropy is achieved by uniform distribution. -/
theorem shannonEntropy_le_log_card (p : Œπ ‚Üí ‚Ñù)
    (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1) :
    shannonEntropy p ‚â§ log (Fintype.card Œπ) := by
  sorry

/-- Entropy of uniform distribution. -/
theorem shannonEntropy_uniform :
    shannonEntropy (Œª _ : Œπ => 1 / Fintype.card Œπ) = log (Fintype.card Œπ) := by
  simp only [shannonEntropy]
  have hcard : (0 : ‚Ñù) < Fintype.card Œπ := Nat.cast_pos.mpr Fintype.card_pos
  have hne : (Fintype.card Œπ : ‚Ñù) ‚â† 0 := ne_of_gt hcard
  have hunif_pos : (0 : ‚Ñù) < 1 / Fintype.card Œπ := by positivity
  have hunif_ne : (1 : ‚Ñù) / Fintype.card Œπ ‚â† 0 := ne_of_gt hunif_pos
  simp only [hunif_ne, ‚ÜìreduceIte, log_div one_ne_zero hne, log_one, zero_sub]
  rw [Finset.sum_const, Finset.card_univ, nsmul_eq_mul]
  field_simp

/-- Entropy of softmax: H(softmax(s, Œ±)) = log Z - Œ± ¬∑ ùîº[s]. -/
theorem shannonEntropy_softmax (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) :
    shannonEntropy (softmax s Œ±) =
    log (partitionFn s Œ±) - Œ± * ‚àë i : Œπ, softmax s Œ± i * s i := by
  simp only [shannonEntropy, softmax, partitionFn]
  have hZ : 0 < ‚àë j : Œπ, exp (Œ± * s j) := partitionFn_pos s Œ±
  have hne : (‚àë j : Œπ, exp (Œ± * s j)) ‚â† 0 := ne_of_gt hZ
  -- Each softmax(i) > 0, so the if-then-else simplifies
  have hsm_pos : ‚àÄ i, exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j) ‚â† 0 := by
    intro i; exact ne_of_gt (div_pos (exp_pos _) hZ)
  simp only [hsm_pos, ‚ÜìreduceIte]
  -- log(exp(Œ±¬∑s·µ¢)/Z) = Œ±¬∑s·µ¢ - log Z
  have hlog : ‚àÄ i, log (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) =
                   Œ± * s i - log (‚àë j : Œπ, exp (Œ± * s j)) := by
    intro i; rw [log_div (ne_of_gt (exp_pos _)) hne, log_exp]
  simp_rw [hlog]
  -- Œ£(exp/Z)¬∑(Œ±s - log Z) = Œ£(exp/Z)¬∑Œ±s - Œ£(exp/Z)¬∑log Z = Œ±¬∑ùîº[s] - log Z
  have hsum1 : ‚àë i : Œπ, exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j) = 1 := by
    rw [‚Üê Finset.sum_div, div_self hne]
  calc -‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i - log (‚àë j : Œπ, exp (Œ± * s j)))
      = -‚àë i : Œπ, ((exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i) -
                   (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * log (‚àë j : Œπ, exp (Œ± * s j))) := by
        congr 1; apply Finset.sum_congr rfl; intros; ring
    _ = -(‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i) -
          ‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * log (‚àë j : Œπ, exp (Œ± * s j))) := by
        rw [Finset.sum_sub_distrib]
    _ = -(‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i) -
          (‚àë i : Œπ, exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * log (‚àë j : Œπ, exp (Œ± * s j))) := by
        rw [‚Üê Finset.sum_mul]
    _ = -(‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i) - 1 * log (‚àë j : Œπ, exp (Œ± * s j))) := by
        rw [hsum1]
    _ = log (‚àë j : Œπ, exp (Œ± * s j)) - ‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * (Œ± * s i) := by ring
    _ = log (‚àë j : Œπ, exp (Œ± * s j)) - ‚àë i : Œπ, Œ± * ((exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * s i) := by
        congr 1; apply Finset.sum_congr rfl; intros; ring
    _ = log (‚àë j : Œπ, exp (Œ± * s j)) - Œ± * ‚àë i : Œπ, (exp (Œ± * s i) / ‚àë j : Œπ, exp (Œ± * s j)) * s i := by
        rw [‚Üê Finset.mul_sum]

/-- Alternative form using log-sum-exp. -/
theorem shannonEntropy_softmax' (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) :
    shannonEntropy (softmax s Œ±) =
    logSumExp s Œ± - Œ± * ‚àë i : Œπ, softmax s Œ± i * s i := by
  simp only [logSumExp]
  exact shannonEntropy_softmax s Œ±

/-- Entropy-regularized objective: G_Œ±(p, s) = ‚ü®s, p‚ü© + (1/Œ±) H(p). -/
noncomputable def entropyRegObjective (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (p : Œπ ‚Üí ‚Ñù) : ‚Ñù :=
  ‚àë i : Œπ, p i * s i + (1 / Œ±) * shannonEntropy p

/-- Fact 5: Softmax maximizes the entropy-regularized objective. -/
theorem softmax_maximizes_entropyReg (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (hŒ± : 0 < Œ±)
    (p : Œπ ‚Üí ‚Ñù) (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1) :
    entropyRegObjective s Œ± p ‚â§ entropyRegObjective s Œ± (softmax s Œ±) := by
  sorry

/-- The maximum value of the entropy-regularized objective. -/
theorem entropyRegObjective_softmax (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (hŒ± : 0 < Œ±) :
    entropyRegObjective s Œ± (softmax s Œ±) = (1 / Œ±) * log (partitionFn s Œ±) := by
  simp only [entropyRegObjective, shannonEntropy_softmax]
  have hne : Œ± ‚â† 0 := ne_of_gt hŒ±
  field_simp
  ring

/-- Softmax is the unique maximizer. -/
theorem softmax_unique_maximizer (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (hŒ± : 0 < Œ±)
    (p : Œπ ‚Üí ‚Ñù) (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1)
    (h_max : entropyRegObjective s Œ± p = entropyRegObjective s Œ± (softmax s Œ±)) :
    p = softmax s Œ± := by
  sorry

/-- KL divergence from q to p. -/
noncomputable def klDiv (p q : Œπ ‚Üí ‚Ñù) : ‚Ñù :=
  ‚àë i : Œπ, if p i = 0 then 0 else p i * log (p i / q i)

/-- KL divergence is non-negative. -/
theorem klDiv_nonneg (p q : Œπ ‚Üí ‚Ñù)
    (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hq_pos : ‚àÄ i, 0 < q i)
    (hp_sum : ‚àë i : Œπ, p i = 1) (hq_sum : ‚àë i : Œπ, q i = 1) :
    0 ‚â§ klDiv p q := by
  sorry

/-- KL divergence is zero iff distributions are equal. -/
theorem klDiv_eq_zero_iff (p q : Œπ ‚Üí ‚Ñù)
    (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hq_pos : ‚àÄ i, 0 < q i)
    (hp_sum : ‚àë i : Œπ, p i = 1) (hq_sum : ‚àë i : Œπ, q i = 1) :
    klDiv p q = 0 ‚Üî p = q := by
  sorry

/-- Softmax minimizes KL divergence from prior weighted by scores. -/
theorem softmax_minimizes_kl_plus_energy (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (hŒ± : 0 < Œ±)
    (p : Œπ ‚Üí ‚Ñù) (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1) :
    klDiv p (Œª _ => 1 / Fintype.card Œπ) - Œ± * ‚àë i, p i * s i ‚â•
    klDiv (softmax s Œ±) (Œª _ => 1 / Fintype.card Œπ) - Œ± * ‚àë i, softmax s Œ± i * s i := by
  sorry

/-- Free energy (from statistical mechanics). -/
noncomputable def freeEnergy (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (p : Œπ ‚Üí ‚Ñù) : ‚Ñù :=
  -‚àë i : Œπ, p i * s i - (1 / Œ±) * shannonEntropy p

/-- Softmax is the Boltzmann distribution: minimizes free energy. -/
theorem softmax_minimizes_freeEnergy (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (hŒ± : 0 < Œ±)
    (p : Œπ ‚Üí ‚Ñù) (hp_nonneg : ‚àÄ i, 0 ‚â§ p i) (hp_sum : ‚àë i : Œπ, p i = 1) :
    freeEnergy s Œ± (softmax s Œ±) ‚â§ freeEnergy s Œ± p := by
  -- This is equivalent to softmax_maximizes_entropyReg (negation)
  simp only [freeEnergy]
  have h := softmax_maximizes_entropyReg s Œ± hŒ± p hp_nonneg hp_sum
  simp only [entropyRegObjective] at h
  linarith

/-- Softmax is an exponential family distribution. -/
theorem softmax_exponential_family (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) (i : Œπ) :
    softmax s Œ± i = exp (Œ± * s i - logSumExp s Œ±) := by
  simp only [softmax, logSumExp]
  rw [exp_sub]
  have h : exp (log (‚àë j : Œπ, exp (Œ± * s j))) = ‚àë j : Œπ, exp (Œ± * s j) :=
    exp_log (partitionFn_pos s Œ±)
  rw [h]

/-- The log-partition function is convex in Œ±. -/
theorem logSumExp_convex (s : Œπ ‚Üí ‚Ñù) :
    ConvexOn ‚Ñù Set.univ (Œª Œ± => logSumExp s Œ±) := by
  sorry

/-- Derivative of log-partition gives expected value. -/
theorem deriv_logSumExp (s : Œπ ‚Üí ‚Ñù) (Œ± : ‚Ñù) :
    deriv (Œª Œ± => logSumExp s Œ±) Œ± = ‚àë i : Œπ, softmax s Œ± i * s i := by
  -- TODO: Requires calculus lemmas for sum of exp derivatives
  -- d/dŒ± log(Z) = Z'/Z where Z = ‚àë exp(Œ± * s_j), Z' = ‚àë s_j * exp(Œ± * s_j)
  sorry

/-- Strong duality: max entropy = min free energy. -/
theorem max_entropy_duality (s : Œπ ‚Üí ‚Ñù) (c : ‚Ñù)
    (Œ± : ‚Ñù) (hŒ± : 0 < Œ±) (h_constraint : ‚àë i : Œπ, softmax s Œ± i * s i = c) :
    shannonEntropy (softmax s Œ±) = log (partitionFn s Œ±) - Œ± * c := by
  rw [shannonEntropy_softmax, h_constraint]

end Softmax
